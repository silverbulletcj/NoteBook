## 第八章 读写外部系统

* 应用的一致性保障
  * 幂等性写
    * 幂等操作可以多次执行，但只会引起一次改变
    * 依赖于幂等性数据汇的应用若要获得精确一次的结果，需要保证在重放时可以覆盖之前写出的结果
  * 事务性写
    * 只有在上次成功的检查点之前计算的结果才会被写入外部数据汇系统；会增加一定延迟
    * 通用的WAL数据汇和一个2PC数据汇实现事务性的数据汇连接器
      * WAL数据汇会将所有结果记录写入应用状态，并在收到检查点完成通知后将它们发送到数据汇系统；利用状态后端缓冲记录，但是无法100%提供精确一次保障
      * 2PC需要数据汇系统支持事务或提供可用来模拟事务的构建；每次生成检查点，都会开启一次事务并将记录附加到该事务中，收到检查点完成通知后，才从事务写入结果
* 内置连接器
  * Apache Kafka数据源连接器
    * Flink Kafka连接器会以并行方式获取事件流，每个并行数据源任务都可以从一个或者多个分区读取数据
    * 创建一个Kafka数据源连接器的构造方法需要三个参数：第一个参数为主题；第二个参数是DeserializationSchema或keyedDeserializationSchema；第三个是一个Properties对象，用于配置连接器内部负责连接和从Kafka读取数据的Kafka客户端
    * 可以配置主题分区开始读取数据的位置：
      * 通过group.ip参数配置的Kafka消费组所记录的最后读取位置，FlinkkafkaConsumer.setStartFromGroupOffsets()
      * 每个分区最早的偏移：FlinkKafkaConsumer.setStartFromEarliest()
      * 每个分区最晚的偏移：FlinkKafkaConsumer.setStartFromLatest()
      * 所有时间戳大于某个给定值的记录：FlinkKafkaConsumer.setStartFromTimestamp(long)
      * 利用一个Map对象为所有分区指定读写位置：FlinkKafkaConsumer.setStartFromSpecificOffsets(Map)
  * Apache Kafka数据汇连接器
    * 创建一个Flink Kafka数据汇的构造方法接收三个参数：第一个参数为英文逗号分隔的Kafka Broker地址字符串；第二个参数是数据写入的主题目标；最后一个参数SerializationSchema用于数据汇的输入类型
    * Kafka数据汇的至少一次保障
      * Flink的检查点功能处于开启状态，应用所有的数据源都是可重置的
      * 如果数据汇连接器写入不成功，则会抛出异常，继而导致应用失败并进行故障恢复
      * 数据汇连接器要在检查点完成前等待Kafka确认记录写入完毕
    * Kafka数据汇的精确一次保障
      * Semantic.NONE，不做任何一致性保障，记录可能会丢失或写入多次
      * Semantic.AT_LEAST_ONCE，保证数据不会丢失，但可能会重复写入
      * Semantic.EXACTLY_ONCE，基于Kafka的事务机制，保证记录可以精确一次写入
  * 文件系统数据源连接器
    * StreamExecutionEnvironment.readFile()方法的参数包括
      * 一个FileInputFormat，负责读数文件内容
      * 要读取的目标路径
      * 目标路径的读取模式，可选项为PROCESS_ONCE或PROCESS_CONTINUOUSLY
      * 针对目标路径周期性扫描的时间间隔
      * FileInputFormat是一个专门用来从文件系统中读文件的InputFormat，读取过程分为两步：
        * 扫描文件系统的路径并为所有满足输入条件的文件创建一些输入划分对象；每个输入划分对象都定义了文件的范围
        * 接收一个输入划分，按照划分中限定的范围读取文件并返回所有对应的记录
  * 文件系统数据汇连接器
    * 假如应用配置了精确一次检查点，且它的所有数据源都能在故障时重置，那么StreamingFileSink就可以提供端到端的一致性保障
      * StreamingFileSink在接收到一条记录后，会将其分配到一个桶中；桶的选择由BucketAssigner来完成，可以为每条记录返回一个用来决定记录写入目录的BucketId；
      * 每个桶对应的目录下都会包含很多分块文件，由StreamingFileSink的多个并行实例进行并发写入，分块路径的规则为：[base-path]/[bucket-path]/part-[task-idx]-[id]
      * RollingPolicy用来决定任务何时创建新的分块文件，可以通过调用构建器的withRollingPolicy()方法对其进行配置
      * StreamingFileSink支持两种分块文件的写入模式：行编码和批量编码
    * 提供精确一次的输出保障，实现方式时采用一种提交协议，基于Flink的检查点机制将文件在不同阶段间转移
  * Apache Cassandra数据汇连接器
    * 将数据集建模成多个类型的列所组成的行表，必须有一个或多个列被定义为复合主键，每一行以主键作为唯一标识
    * Cassandra数据汇的创建和配置都是通过构建器完成
* 实现自定义数据源函数
  * SourceFunction和RichSourceFuntion可用于定义非并行的数据源连接器，即只能以单任务运行
  * ParallelSourceFunction和RichParallelSourceFunction可用于定义能同时运行多个任务实例的数据源连接器
  * SourceFuntion和ParallelSourceFunction中定义了两个方法
    * void run(SourceContext<T> ctx)负责执行具体的记录读入或接收工作
    * void cancel()在应用被取消或关闭时调用
  * 可重置的数据源函数
    * 支持输出重放的数据源函数需要和Flink的检查点机制集成，并在生成检查点时持久化所有当前的读取位置
    * 可重置的数据源函数需要实现CheckpointedFunction接口，并把所有读取偏移和相关的元数据存入算子状态列表或算子联合列表状态中
    * 要确保单线程运行的SourceFunction.run()不会在检查点生成的过程中推进偏移
  * 数据源函数、时间戳和水位线
    * 数据源分配时间戳和发出水位线需要依赖内部的SourceContext对象，该对象提供以下方法
      * def collectWithTimestamp(T record, long timestamp)用来发出记录和与之关联的时间戳
      * def emitWatermark(Watermark watermark)用来发出传入的水位线
* 实现自定义数据汇函数
  * DataStream API为我们提供一个专门的SinkFunction接口以及对应的RichSinkFunction抽象类
    * void invoke(IN value, Context ctx)利用Context对象访问当前处理时间，水位线以及记录的时间戳
    * 为了实现端到端的精确一次语义，应用的连接器需要是幂等性的或支持事务
  * 幂等性数据汇连接器
    * 实施前提
      * 结果数据中用于幂等更新的键是确定的
      * 外部系统支持按键更新
  * 事务性数据汇连接器
    * 事务性数据汇连接器可能只会在检查点成功完成后才将数据写入外部系统中
    * Flink DataStream API提供两个模板
      * GenericWriteAheadSink模板：将经由各个检查点分段后的接收记录以追加的形式写入WAL中
        * 继承自GenericWriteAheadSink的算子需要在构造方法内提供三个参数
          * CkeckpointCommitter
          * TypeSerializer
          * 传递给CheckpointCommitter，用于应用重启后标识提交信息的任务ID
        * 还需要sendValues()方法将已完成检查点对应的记录写入外部存储系统
        * 两种情况会导致记录发出多次
          * 程序在任务运行sendValues()方法时发生故障
          * 所有记录都已成功写入，sendValues()方法返回true，但程序在调用CheckpointCommitter前出现故障
      * TwoPhaseCommitSinkFunction模板
        * 2PC协议的投票阶段始于JobManager对某个检查点进行初始化并向应用数据汇中注入分隔符
        * 算子收到分隔符时，将内部状态写入检查点，之后向JobManager发送确认消息
        * 数据汇收到分隔符时，将内部状态持久化，向JobManager发送检查点的确认消息
        * Jobmanager在收到从所有任务实例返回的检查点成功消息之后，就会将检查点的完成通知发送至所有相关任务
        * 数据汇在收到通知后，就会提交事务
        * 需要实现的五个方法：
          * beginTransaction()：TXN用于启动一个新的事务并返回事务标识
          * invoke(txn: TXN, value: IN, context: Context[])：将传入值写入当前事务中
          * preCommit(txn: TXN)：预提交事务
          * commit(txn: TXN)：提交指定事务
          * abort(txn: TXN)：终止给定事务
* 异步访问外部系统

